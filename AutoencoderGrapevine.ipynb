{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c32d614",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc0d4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256\n",
    "CHANNELS=3\n",
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"LeafDisease\",\n",
    "    shuffle= True,\n",
    "    image_size = (IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, _ in dataset.take(1):  # Adjust the number of batches as needed\n",
    "    min_value = tf.reduce_min(images).numpy()\n",
    "    max_value = tf.reduce_max(images).numpy()\n",
    "    print(\"Minimum value:\", min_value)\n",
    "    print(\"Maximum value:\", max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad62a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb998cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9865d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in dataset.take(1):\n",
    "    print(image_batch.shape)\n",
    "    print(labels_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for image_batch, labels_batch in dataset.take(1):\n",
    "    for i in range(12):\n",
    "        ax = plt.subplot(3, 4, i + 1)\n",
    "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels_batch[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "len(dataset)*train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f083f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dataset.take(899)\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4806c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = dataset.skip(899)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size=0.1\n",
    "len(dataset)*val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2eedb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = test_ds.take(112)\n",
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_ds.skip(112)\n",
    "len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "    \n",
    "    ds_size = len(ds)\n",
    "    \n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=12)\n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7befd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac80c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_and_rescale = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc36408",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y)\n",
    ").prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa15489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Correct super() call without arguments\n",
    "\n",
    "        # N, 1, 28, 28\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(16, (3, 3), strides=2, padding=\"same\", activation=\"relu\"),  # -> N, 16, 14, 14\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), strides=2, padding=\"same\", activation=\"relu\"),  # -> N, 32, 7, 7\n",
    "            tf.keras.layers.Conv2D(64, (7, 7), activation=\"relu\")  # -> N, 64, 1, 1\n",
    "        ])\n",
    "\n",
    "        # N, 64, 1, 1\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2DTranspose(32, (7, 7), activation=\"relu\"),  # -> N, 32, 7, 7\n",
    "            tf.keras.layers.Conv2DTranspose(16, (3, 3), strides=2, padding=\"same\", output_padding=1, activation=\"relu\"),  # N, 16, 14, 14\n",
    "            tf.keras.layers.Conv2DTranspose(1, (3, 3), strides=2, padding=\"same\", output_padding=1, activation=\"sigmoid\")  # N, 1, 28, 28\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace95b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "\n",
    "# Define the loss function (Mean Squared Error)\n",
    "loss_function = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the optimizer (Adam optimizer)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc6b0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "outputs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    x = 0\n",
    "    for img, _ in dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            recon = model(img)\n",
    "            loss = tf.keras.losses.MeanSquaredError()(img, recon)\n",
    "        \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        x = x+1\n",
    "        print(x)\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss:.4f}')\n",
    "    outputs.append((epoch, img, recon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef9440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for k in range(0, num_epochs, 4):\n",
    "    plt.figure(figsize=(40, 30))\n",
    "    plt.gray()\n",
    "    imgs = outputs[k][1].numpy()\n",
    "    recon = outputs[k][2].numpy()\n",
    "    for i, item in enumerate(imgs):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        plt.subplot(2, 9, i + 1)\n",
    "        # item: (height, width, channels)\n",
    "        plt.imshow(item[:, :, 0], cmap='gray')\n",
    "        \n",
    "    for i, item in enumerate(recon):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        plt.subplot(2, 9, 9 + i + 1)  # row_length + i + 1\n",
    "        # item: (height, width, channels)\n",
    "        plt.imshow(item[:, :, 0], cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a9952",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, num_epochs, 4):\n",
    "    plt.figure(figsize=(9, 2))\n",
    "    recon = outputs[k][2].numpy()\n",
    "    \n",
    "    for i in range(9):\n",
    "        plt.subplot(2, 9, i + 1)\n",
    "        plt.imshow(recon[i, :, :], cmap='gray')\n",
    "        plt.axis('off')  # Remove axis labels and ticks\n",
    "\n",
    "plt.show()  # Display the reconstructed images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a66652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataset(train_folder, test_folder, split_ratio=0.2):\n",
    "    # Create the test dataset folder if it doesn't exist\n",
    "    if not os.path.exists(test_folder):\n",
    "        os.makedirs(test_folder)\n",
    "\n",
    "    # Iterate through each class in the train dataset folder\n",
    "    for class_folder in os.listdir(train_folder):\n",
    "        # Skip hidden files (e.g., .DS_Store)\n",
    "        if class_folder.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        class_path = os.path.join(train_folder, class_folder)\n",
    "        \n",
    "        # Create the corresponding class folder in the test dataset\n",
    "        test_class_path = os.path.join(test_folder, class_folder)\n",
    "        if not os.path.exists(test_class_path):\n",
    "            os.makedirs(test_class_path)\n",
    "\n",
    "        # Get a list of all images in the class folder\n",
    "        images = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "        # Calculate the number of images to move to the test dataset\n",
    "        num_test_images = int(split_ratio * len(images))\n",
    "\n",
    "        # Randomly select images for the test dataset\n",
    "        test_images = random.sample(images, num_test_images)\n",
    "\n",
    "        # Move selected images to the test dataset folder\n",
    "        for image in test_images:\n",
    "            src_path = os.path.join(class_path, image)\n",
    "            dest_path = os.path.join(test_class_path, image)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your train and test dataset folder paths\n",
    "    train_dataset_folder = \"/Users/aayushrangra/Desktop/GrapeVine_Disease_Detection/LeafDisease\"\n",
    "    test_dataset_folder = \"/Users/aayushrangra/Desktop/GrapeVine_Disease_Detection/TestData\"\n",
    "\n",
    "    # Set the split ratio (e.g., 0.2 for 20% test, 80% train)\n",
    "    split_ratio = 0.2\n",
    "\n",
    "    # Create the test dataset\n",
    "    create_test_dataset(train_dataset_folder, test_dataset_folder, split_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58793779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7336f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_dataset_length(dataset_folder):\n",
    "    # Iterate through each class in the dataset folder\n",
    "    for class_folder in os.listdir(dataset_folder):\n",
    "        # Skip hidden files (e.g., .DS_Store)\n",
    "        if class_folder.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        class_path = os.path.join(dataset_folder, class_folder)\n",
    "\n",
    "        # Get a list of all images in the class folder\n",
    "        images = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "        # Check the length of the images list\n",
    "        num_images = len(images)\n",
    "\n",
    "        # Output the number of images for each class\n",
    "        print(f\"Class '{class_folder}' has {num_images} images.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your dataset folder path\n",
    "    Test = \"/Users/aayushrangra/Desktop/GrapeVine_Disease_Detection/TestData\"\n",
    "\n",
    "    # Check the length of the dataset\n",
    "    check_dataset_length(Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82526a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ef7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(10, 2)\n",
    "targets = torch.randn(10, 1)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(inputs)\n",
    "\n",
    "# calculate the loss\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(outputs, targets)\n",
    "\n",
    "# backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# optimization step\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85908b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=5000\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # calculate the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "    # print the loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training, you can use the trained model for predictions\n",
    "# For example:\n",
    "test_input = torch.randn(1, 2)\n",
    "predicted_output = model(test_input)\n",
    "print(\"Predicted Output:\", predicted_output.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "import torch_optimizer as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c1f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from diffgrad import DiffGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "947c3742",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'register' from 'tensorflow.keras.optimizers' (/Applications/anaconda3/lib/python3.9/site-packages/keras/api/_v2/keras/optimizers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'register' from 'tensorflow.keras.optimizers' (/Applications/anaconda3/lib/python3.9/site-packages/keras/api/_v2/keras/optimizers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6487cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 03:46:52.236555: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret optimizer identifier: <diffgrad.DiffGrad object at 0x7fd719e03430>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m DiffGrad()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# compile the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean_squared_error\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# number of training epochs\u001b[39;00m\n\u001b[1;32m     18\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/keras/optimizers/__init__.py:171\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m deserialize(config)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    172\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not interpret optimizer identifier: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(identifier))\n",
      "\u001b[0;31mValueError\u001b[0m: Could not interpret optimizer identifier: <diffgrad.DiffGrad object at 0x7fd719e03430>"
     ]
    }
   ],
   "source": [
    "# Assuming you have your input data and target data as 'inputs' and 'targets'\n",
    "inputs = tf.random.normal((10, 2))\n",
    "targets = tf.random.normal((10, 1))\n",
    "\n",
    "# model definition\n",
    "model = models.Sequential([\n",
    "    layers.Dense(3, activation='relu', input_shape=(2,)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "# optimizer definition\n",
    "optimizer = DiffGrad()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# number of training epochs\n",
    "num_epochs = 1000\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # calculate the loss\n",
    "    loss = model.evaluate(inputs, targets, verbose=0)\n",
    "    \n",
    "    # backward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = model(inputs, training=True)\n",
    "    \n",
    "    # optimization step\n",
    "    gradients = tape.gradient(current_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # print the loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n",
    "\n",
    "# After training, you can use the trained model for predictions\n",
    "# For example:\n",
    "test_input = tf.random.normal((1, 2))\n",
    "predicted_output = model.predict(test_input)\n",
    "print(\"Predicted Output:\", predicted_output[0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92178381",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Users/aayushrangra/Desktop/GrapeVine_Disease_Detection/custom_dataset/train/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m             shutil\u001b[38;5;241m.\u001b[39mmove(src_path, dest_path)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43mcreate_validation_set\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/aayushrangra/Desktop/GrapeVine_Disease_Detection/custom_dataset/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/aayushrangra/Desktop/GrapeVine_Disease_Detection/custom_dataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_percent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m, in \u001b[0;36mcreate_validation_set\u001b[0;34m(data_dir, validation_dir, validation_percent)\u001b[0m\n\u001b[1;32m     17\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(validation_class_path)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# List all files in the class folder\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate the number of files to move to the validation set\u001b[39;00m\n\u001b[1;32m     23\u001b[0m num_validation_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(validation_percent \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(files))\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/Users/aayushrangra/Desktop/GrapeVine_Disease_Detection/custom_dataset/train/.DS_Store'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def create_validation_set(data_dir, validation_dir, validation_percent=0.1):\n",
    "    # Create the validation set directory if it doesn't exist\n",
    "    if not os.path.exists(validation_dir):\n",
    "        os.makedirs(validation_dir)\n",
    "\n",
    "    # Iterate through each class folder in the training set\n",
    "    for class_folder in os.listdir(data_dir):\n",
    "        class_path = os.path.join(data_dir, class_folder)\n",
    "        validation_class_path = os.path.join(validation_dir, class_folder)\n",
    "\n",
    "        # Create the class folder in the validation set\n",
    "        if not os.path.exists(validation_class_path):\n",
    "            os.makedirs(validation_class_path)\n",
    "\n",
    "        # List all files in the class folder\n",
    "        files = os.listdir(class_path)\n",
    "\n",
    "        # Calculate the number of files to move to the validation set\n",
    "        num_validation_files = int(validation_percent * len(files))\n",
    "\n",
    "        # Randomly select files for validation\n",
    "        validation_files = random.sample(files, num_validation_files)\n",
    "\n",
    "        # Move selected files to the validation set folder\n",
    "        for file in validation_files:\n",
    "            src_path = os.path.join(class_path, file)\n",
    "            dest_path = os.path.join(validation_class_path, file)\n",
    "            shutil.move(src_path, dest_path)\n",
    "\n",
    "# Example usage\n",
    "create_validation_set('/Users/aayushrangra/Desktop/GrapeVine_Disease_Detection/custom_dataset/train', '/Users/aayushrangra/Desktop/GrapeVine_Disease_Detection/custom_dataset', validation_percent=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cca66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
